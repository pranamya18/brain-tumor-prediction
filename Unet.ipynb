{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ8XVQBzPRV9",
        "outputId": "6db3c32d-15b8-424a-e9a4-d67e8f8f5872"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFO651qr_pok",
        "outputId": "f8eb964c-ca6e-44d3-c597-42e5b2f4a30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "#from metrics import dice_loss, dice_coef, iou\n",
        "#from unet import build_unet\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2VqsGxLbNuVZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "VbR1C-z3_yGC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(inputs, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "iB4qV90bAgHa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block(inputs, num_filters):\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_q9m5_DXA_mI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, 2 , strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "wUokDGzCINgC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from json import decoder\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "    # print(s1.shape, s2.shape, s3.shape, s4.shape)\n",
        "    # print(p1.shape, p2.shape, p3.shape, p4.shape)\n",
        "\n",
        "    b1 = conv_block(p4, 1024) #Bridge\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "R6ez5T2bB4M7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_shape = (256, 256, 3)\n",
        "    model = build_unet(input_shape)\n",
        "    model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFN4EaEYDEde",
        "outputId": "d3a80d24-d8e9-4725-dbc9-11f15f7accce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"U-Net\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 256, 256, 64)         1792      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 256, 256, 64)         256       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)         36928     ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 256, 256, 64)         256       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 256, 256, 64)         0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 128, 128, 64)         0         ['activation_1[0][0]']        \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 128)        73856     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 128, 128, 128)        512       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 128)        147584    ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 128, 128, 128)        512       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 128, 128, 128)        0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)          0         ['activation_3[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 64, 64, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 64, 64, 256)          1024      ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 256)          590080    ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 64, 64, 256)          1024      ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 64, 64, 256)          0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 32, 32, 256)          0         ['activation_5[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 32, 32, 512)          2359808   ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 32, 32, 512)          0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 512)          0         ['activation_7[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 16, 16, 1024)         4719616   ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 16, 16, 1024)         9438208   ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 16, 16, 1024)         4096      ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 16, 16, 1024)         0         ['batch_normalization_9[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 32, 32, 512)          2097664   ['activation_9[0][0]']        \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 32, 32, 1024)         0         ['conv2d_transpose[0][0]',    \n",
            "                                                                     'activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 32, 32, 512)          4719104   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 32, 32, 512)          2048      ['conv2d_10[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 32, 32, 512)          2359808   ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 32, 32, 512)          2048      ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 32, 32, 512)          0         ['batch_normalization_11[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 64, 64, 256)          524544    ['activation_11[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 64, 64, 512)          0         ['conv2d_transpose_1[0][0]',  \n",
            " )                                                                   'activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 64, 64, 256)          1179904   ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 64, 64, 256)          1024      ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 64, 64, 256)          590080    ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 64, 64, 256)          1024      ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 64, 64, 256)          0         ['batch_normalization_13[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 128, 128, 128)        131200    ['activation_13[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 128, 128, 256)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 128, 128, 128)        295040    ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 128, 128, 128)        512       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 128, 128, 128)        147584    ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 256, 256, 64)         32832     ['activation_15[0][0]']       \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 256, 256, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 256, 256, 64)         73792     ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 256, 256, 64)         256       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 256, 256, 64)         36928     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 256, 256, 64)         256       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 256, 256, 64)         0         ['batch_normalization_17[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 256, 256, 1)          65        ['activation_17[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31055297 (118.47 MB)\n",
            "Trainable params: 31043521 (118.42 MB)\n",
            "Non-trainable params: 11776 (46.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H = 256\n",
        "W = 256\n"
      ],
      "metadata": {
        "id": "56ZQccqWOUHI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eVrEk3_TOtfe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, split=0.2):\n",
        "    images = sorted(glob(os.path.join(path, \"images\", \"*.png\")))\n",
        "    masks = sorted(glob(os.path.join(path, \"masks\", \"*.png\")))\n",
        "    split_size = int(len(images) * split)\n",
        "    #print(split_size)\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=split_size, random_state=42)\n",
        "\n",
        "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "    train_y, test_y = train_test_split(train_y, test_size=split_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)"
      ],
      "metadata": {
        "id": "Lg_RaQkcQm8t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x"
      ],
      "metadata": {
        "id": "GrJILIPBTk-c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "rRe8A-MEUUx7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "    x,y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "YDWmC-EmVELk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_dataset(X,Y,batch=2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(10)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "bMe3zvMJXSQh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n"
      ],
      "metadata": {
        "id": "712V1uwKb-DE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "wPoPrJTVp_Pa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.api._v2.keras import callbacks\n",
        "from tensorflow.python import train\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    create_dir(\"/content/drive/MyDrive/files\")\n",
        "\n",
        "    batch_size = 8\n",
        "    lr = 1e-4\n",
        "    num_epochs = 500\n",
        "    model_path = \"/content/drive/MyDrive/files/model.h5\"\n",
        "    csv_path = \"/content/drive/MyDrive/files/data.csv\"\n",
        "    dataset_path = \"/content/drive/MyDrive/brain\"\n",
        "    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
        "    # print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    # print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "    # print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n",
        "\n",
        "\n",
        "    model = build_unet((H, W, 3))\n",
        "    model.compile(loss=dice_loss, optimizer=tf.keras.optimizers.Adam(lr), metrics=[dice_coef])\n",
        "    callbacks = [\n",
        "        callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        callbacks.CSVLogger(csv_path),\n",
        "        callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kubJlucOx7h",
        "outputId": "ed2e6fd8-c6c4-4c10-dddb-f6ada8285104"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.8675 - dice_coef: 0.1325\n",
            "Epoch 1: val_loss improved from inf to 0.95527, saving model to /content/drive/MyDrive/files/model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 637s 3s/step - loss: 0.8675 - dice_coef: 0.1325 - val_loss: 0.9553 - val_dice_coef: 0.0447 - lr: 1.0000e-04\n",
            "Epoch 2/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.7496 - dice_coef: 0.2504\n",
            "Epoch 2: val_loss improved from 0.95527 to 0.93420, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 125s 544ms/step - loss: 0.7496 - dice_coef: 0.2504 - val_loss: 0.9342 - val_dice_coef: 0.0656 - lr: 1.0000e-04\n",
            "Epoch 3/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.6431 - dice_coef: 0.3569\n",
            "Epoch 3: val_loss improved from 0.93420 to 0.63896, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 129s 561ms/step - loss: 0.6431 - dice_coef: 0.3569 - val_loss: 0.6390 - val_dice_coef: 0.3603 - lr: 1.0000e-04\n",
            "Epoch 4/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.5257 - dice_coef: 0.4743\n",
            "Epoch 4: val_loss improved from 0.63896 to 0.51742, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 568ms/step - loss: 0.5257 - dice_coef: 0.4743 - val_loss: 0.5174 - val_dice_coef: 0.4813 - lr: 1.0000e-04\n",
            "Epoch 5/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.4288 - dice_coef: 0.5712\n",
            "Epoch 5: val_loss improved from 0.51742 to 0.41792, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 569ms/step - loss: 0.4288 - dice_coef: 0.5712 - val_loss: 0.4179 - val_dice_coef: 0.5829 - lr: 1.0000e-04\n",
            "Epoch 6/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.3625 - dice_coef: 0.6375\n",
            "Epoch 6: val_loss did not improve from 0.41792\n",
            "230/230 [==============================] - 128s 553ms/step - loss: 0.3625 - dice_coef: 0.6375 - val_loss: 0.5482 - val_dice_coef: 0.4521 - lr: 1.0000e-04\n",
            "Epoch 7/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.3254 - dice_coef: 0.6746\n",
            "Epoch 7: val_loss improved from 0.41792 to 0.34905, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 570ms/step - loss: 0.3254 - dice_coef: 0.6746 - val_loss: 0.3491 - val_dice_coef: 0.6507 - lr: 1.0000e-04\n",
            "Epoch 8/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.2832 - dice_coef: 0.7168\n",
            "Epoch 8: val_loss improved from 0.34905 to 0.32455, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 571ms/step - loss: 0.2832 - dice_coef: 0.7168 - val_loss: 0.3245 - val_dice_coef: 0.6748 - lr: 1.0000e-04\n",
            "Epoch 9/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.2616 - dice_coef: 0.7384\n",
            "Epoch 9: val_loss improved from 0.32455 to 0.30087, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 132s 573ms/step - loss: 0.2616 - dice_coef: 0.7384 - val_loss: 0.3009 - val_dice_coef: 0.6988 - lr: 1.0000e-04\n",
            "Epoch 10/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.2414 - dice_coef: 0.7586\n",
            "Epoch 10: val_loss improved from 0.30087 to 0.27737, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 570ms/step - loss: 0.2414 - dice_coef: 0.7586 - val_loss: 0.2774 - val_dice_coef: 0.7194 - lr: 1.0000e-04\n",
            "Epoch 11/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.2175 - dice_coef: 0.7825\n",
            "Epoch 11: val_loss improved from 0.27737 to 0.26308, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 570ms/step - loss: 0.2175 - dice_coef: 0.7825 - val_loss: 0.2631 - val_dice_coef: 0.7370 - lr: 1.0000e-04\n",
            "Epoch 12/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1980 - dice_coef: 0.8020\n",
            "Epoch 12: val_loss did not improve from 0.26308\n",
            "230/230 [==============================] - 128s 554ms/step - loss: 0.1980 - dice_coef: 0.8020 - val_loss: 0.2711 - val_dice_coef: 0.7283 - lr: 1.0000e-04\n",
            "Epoch 13/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1895 - dice_coef: 0.8105\n",
            "Epoch 13: val_loss did not improve from 0.26308\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.1895 - dice_coef: 0.8105 - val_loss: 0.2749 - val_dice_coef: 0.7250 - lr: 1.0000e-04\n",
            "Epoch 14/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1872 - dice_coef: 0.8128\n",
            "Epoch 14: val_loss improved from 0.26308 to 0.25832, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 132s 574ms/step - loss: 0.1872 - dice_coef: 0.8128 - val_loss: 0.2583 - val_dice_coef: 0.7417 - lr: 1.0000e-04\n",
            "Epoch 15/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1725 - dice_coef: 0.8275\n",
            "Epoch 15: val_loss improved from 0.25832 to 0.25393, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 132s 573ms/step - loss: 0.1725 - dice_coef: 0.8275 - val_loss: 0.2539 - val_dice_coef: 0.7457 - lr: 1.0000e-04\n",
            "Epoch 16/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1823 - dice_coef: 0.8177\n",
            "Epoch 16: val_loss improved from 0.25393 to 0.24313, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 570ms/step - loss: 0.1823 - dice_coef: 0.8177 - val_loss: 0.2431 - val_dice_coef: 0.7566 - lr: 1.0000e-04\n",
            "Epoch 17/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1589 - dice_coef: 0.8411\n",
            "Epoch 17: val_loss improved from 0.24313 to 0.22773, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 569ms/step - loss: 0.1589 - dice_coef: 0.8411 - val_loss: 0.2277 - val_dice_coef: 0.7717 - lr: 1.0000e-04\n",
            "Epoch 18/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1550 - dice_coef: 0.8450\n",
            "Epoch 18: val_loss did not improve from 0.22773\n",
            "230/230 [==============================] - 128s 554ms/step - loss: 0.1550 - dice_coef: 0.8450 - val_loss: 0.2304 - val_dice_coef: 0.7691 - lr: 1.0000e-04\n",
            "Epoch 19/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1406 - dice_coef: 0.8594\n",
            "Epoch 19: val_loss improved from 0.22773 to 0.22306, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 570ms/step - loss: 0.1406 - dice_coef: 0.8594 - val_loss: 0.2231 - val_dice_coef: 0.7764 - lr: 1.0000e-04\n",
            "Epoch 20/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1393 - dice_coef: 0.8607\n",
            "Epoch 20: val_loss did not improve from 0.22306\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.1393 - dice_coef: 0.8607 - val_loss: 0.2260 - val_dice_coef: 0.7743 - lr: 1.0000e-04\n",
            "Epoch 21/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1387 - dice_coef: 0.8613\n",
            "Epoch 21: val_loss improved from 0.22306 to 0.21836, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 132s 574ms/step - loss: 0.1387 - dice_coef: 0.8613 - val_loss: 0.2184 - val_dice_coef: 0.7825 - lr: 1.0000e-04\n",
            "Epoch 22/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1446 - dice_coef: 0.8554\n",
            "Epoch 22: val_loss did not improve from 0.21836\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.1446 - dice_coef: 0.8554 - val_loss: 0.2574 - val_dice_coef: 0.7427 - lr: 1.0000e-04\n",
            "Epoch 23/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1280 - dice_coef: 0.8720\n",
            "Epoch 23: val_loss improved from 0.21836 to 0.21684, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 571ms/step - loss: 0.1280 - dice_coef: 0.8720 - val_loss: 0.2168 - val_dice_coef: 0.7828 - lr: 1.0000e-04\n",
            "Epoch 24/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1197 - dice_coef: 0.8803\n",
            "Epoch 24: val_loss improved from 0.21684 to 0.21286, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 132s 571ms/step - loss: 0.1197 - dice_coef: 0.8803 - val_loss: 0.2129 - val_dice_coef: 0.7865 - lr: 1.0000e-04\n",
            "Epoch 25/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1202 - dice_coef: 0.8798\n",
            "Epoch 25: val_loss did not improve from 0.21286\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.1202 - dice_coef: 0.8798 - val_loss: 0.2191 - val_dice_coef: 0.7811 - lr: 1.0000e-04\n",
            "Epoch 26/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1232 - dice_coef: 0.8768\n",
            "Epoch 26: val_loss did not improve from 0.21286\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.1232 - dice_coef: 0.8768 - val_loss: 0.2559 - val_dice_coef: 0.7440 - lr: 1.0000e-04\n",
            "Epoch 27/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1277 - dice_coef: 0.8723\n",
            "Epoch 27: val_loss did not improve from 0.21286\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.1277 - dice_coef: 0.8723 - val_loss: 0.2344 - val_dice_coef: 0.7652 - lr: 1.0000e-04\n",
            "Epoch 28/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1162 - dice_coef: 0.8838\n",
            "Epoch 28: val_loss did not improve from 0.21286\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.1162 - dice_coef: 0.8838 - val_loss: 0.2484 - val_dice_coef: 0.7514 - lr: 1.0000e-04\n",
            "Epoch 29/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1266 - dice_coef: 0.8734\n",
            "Epoch 29: val_loss did not improve from 0.21286\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "230/230 [==============================] - 128s 554ms/step - loss: 0.1266 - dice_coef: 0.8734 - val_loss: 0.2135 - val_dice_coef: 0.7862 - lr: 1.0000e-04\n",
            "Epoch 30/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.1045 - dice_coef: 0.8955\n",
            "Epoch 30: val_loss improved from 0.21286 to 0.18944, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 571ms/step - loss: 0.1045 - dice_coef: 0.8955 - val_loss: 0.1894 - val_dice_coef: 0.8108 - lr: 1.0000e-05\n",
            "Epoch 31/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0971 - dice_coef: 0.9029\n",
            "Epoch 31: val_loss improved from 0.18944 to 0.18897, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 570ms/step - loss: 0.0971 - dice_coef: 0.9029 - val_loss: 0.1890 - val_dice_coef: 0.8108 - lr: 1.0000e-05\n",
            "Epoch 32/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0939 - dice_coef: 0.9061\n",
            "Epoch 32: val_loss improved from 0.18897 to 0.18824, saving model to /content/drive/MyDrive/files/model.h5\n",
            "230/230 [==============================] - 131s 569ms/step - loss: 0.0939 - dice_coef: 0.9061 - val_loss: 0.1882 - val_dice_coef: 0.8114 - lr: 1.0000e-05\n",
            "Epoch 33/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0916 - dice_coef: 0.9084\n",
            "Epoch 33: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 554ms/step - loss: 0.0916 - dice_coef: 0.9084 - val_loss: 0.1884 - val_dice_coef: 0.8112 - lr: 1.0000e-05\n",
            "Epoch 34/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0897 - dice_coef: 0.9103\n",
            "Epoch 34: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0897 - dice_coef: 0.9103 - val_loss: 0.1885 - val_dice_coef: 0.8111 - lr: 1.0000e-05\n",
            "Epoch 35/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0877 - dice_coef: 0.9123\n",
            "Epoch 35: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0877 - dice_coef: 0.9123 - val_loss: 0.1883 - val_dice_coef: 0.8112 - lr: 1.0000e-05\n",
            "Epoch 36/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0862 - dice_coef: 0.9138\n",
            "Epoch 36: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 556ms/step - loss: 0.0862 - dice_coef: 0.9138 - val_loss: 0.1887 - val_dice_coef: 0.8108 - lr: 1.0000e-05\n",
            "Epoch 37/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0847 - dice_coef: 0.9153\n",
            "Epoch 37: val_loss did not improve from 0.18824\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "230/230 [==============================] - 128s 554ms/step - loss: 0.0847 - dice_coef: 0.9153 - val_loss: 0.1894 - val_dice_coef: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 38/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0828 - dice_coef: 0.9172\n",
            "Epoch 38: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0828 - dice_coef: 0.9172 - val_loss: 0.1896 - val_dice_coef: 0.8099 - lr: 1.0000e-06\n",
            "Epoch 39/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0824 - dice_coef: 0.9176\n",
            "Epoch 39: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0824 - dice_coef: 0.9176 - val_loss: 0.1899 - val_dice_coef: 0.8096 - lr: 1.0000e-06\n",
            "Epoch 40/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0821 - dice_coef: 0.9179\n",
            "Epoch 40: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0821 - dice_coef: 0.9179 - val_loss: 0.1902 - val_dice_coef: 0.8093 - lr: 1.0000e-06\n",
            "Epoch 41/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0819 - dice_coef: 0.9181\n",
            "Epoch 41: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.0819 - dice_coef: 0.9181 - val_loss: 0.1903 - val_dice_coef: 0.8092 - lr: 1.0000e-06\n",
            "Epoch 42/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0817 - dice_coef: 0.9183\n",
            "Epoch 42: val_loss did not improve from 0.18824\n",
            "\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0817 - dice_coef: 0.9183 - val_loss: 0.1904 - val_dice_coef: 0.8091 - lr: 1.0000e-06\n",
            "Epoch 43/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0814 - dice_coef: 0.9186\n",
            "Epoch 43: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0814 - dice_coef: 0.9186 - val_loss: 0.1902 - val_dice_coef: 0.8093 - lr: 1.0000e-07\n",
            "Epoch 44/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0813 - dice_coef: 0.9187\n",
            "Epoch 44: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 554ms/step - loss: 0.0813 - dice_coef: 0.9187 - val_loss: 0.1901 - val_dice_coef: 0.8094 - lr: 1.0000e-07\n",
            "Epoch 45/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0813 - dice_coef: 0.9187\n",
            "Epoch 45: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.0813 - dice_coef: 0.9187 - val_loss: 0.1901 - val_dice_coef: 0.8095 - lr: 1.0000e-07\n",
            "Epoch 46/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0813 - dice_coef: 0.9187\n",
            "Epoch 46: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.0813 - dice_coef: 0.9187 - val_loss: 0.1900 - val_dice_coef: 0.8095 - lr: 1.0000e-07\n",
            "Epoch 47/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0812 - dice_coef: 0.9188\n",
            "Epoch 47: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.0812 - dice_coef: 0.9188 - val_loss: 0.1900 - val_dice_coef: 0.8095 - lr: 1.0000e-07\n",
            "Epoch 48/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0812 - dice_coef: 0.9188\n",
            "Epoch 48: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.0812 - dice_coef: 0.9188 - val_loss: 0.1901 - val_dice_coef: 0.8095 - lr: 1.0000e-07\n",
            "Epoch 49/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0812 - dice_coef: 0.9188\n",
            "Epoch 49: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0812 - dice_coef: 0.9188 - val_loss: 0.1901 - val_dice_coef: 0.8095 - lr: 1.0000e-07\n",
            "Epoch 50/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0811 - dice_coef: 0.9189\n",
            "Epoch 50: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0811 - dice_coef: 0.9189 - val_loss: 0.1901 - val_dice_coef: 0.8094 - lr: 1.0000e-07\n",
            "Epoch 51/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0811 - dice_coef: 0.9189\n",
            "Epoch 51: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 127s 554ms/step - loss: 0.0811 - dice_coef: 0.9189 - val_loss: 0.1901 - val_dice_coef: 0.8094 - lr: 1.0000e-07\n",
            "Epoch 52/500\n",
            "230/230 [==============================] - ETA: 0s - loss: 0.0811 - dice_coef: 0.9189\n",
            "Epoch 52: val_loss did not improve from 0.18824\n",
            "230/230 [==============================] - 128s 555ms/step - loss: 0.0811 - dice_coef: 0.9189 - val_loss: 0.1901 - val_dice_coef: 0.8094 - lr: 1.0000e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import f1_score, jaccard_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HWxXf-moPLpk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "R5c2zC_6PuiX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(image, mask, y_pred, save_image_path):\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    mask = np.concatenate([mask, mask, mask], axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis=-1)\n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)\n",
        "    y_pred = y_pred * 255\n",
        "\n",
        "    line = np.ones((H, 10, 3)) * 255\n",
        "\n",
        "    cat_images = np.concatenate([image, mask, y_pred, line], axis=1)\n",
        "    cv2.imwrite(save_image_path, cat_images)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LjP254FU44e"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    create_dir(\"/content/drive/MyDrive/files/results\")\n",
        "\n",
        "    with CustomObjectScope({'dice_loss': dice_loss, 'dice_coef': dice_coef}):\n",
        "        model = tf.keras.models.load_model(\"/content/drive/MyDrive/files/model.h5\")\n",
        "\n",
        "    #model.summary()\n",
        "    SCORE = []\n",
        "    for x,y in tqdm(zip(test_x, test_y), total=len(test_y)):\n",
        "       name = x.split(\"/\")[-1]\n",
        "       image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "       image = cv2.resize(image, (W, H))\n",
        "       x = image / 255.0\n",
        "       x = np.expand_dims(x, axis=0)\n",
        "\n",
        "       mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "       mask = cv2.resize(mask, (W, H))\n",
        "\n",
        "\n",
        "       y_pred = model.predict(x,verbose=0)[0]\n",
        "       y_pred = np.squeeze(y_pred,axis=-1)\n",
        "       y_pred = y_pred >= 0.5\n",
        "       y_pred = y_pred.astype(np.int32)\n",
        "\n",
        "       save_image_path = \"/content/drive/MyDrive/files/results/\"+name\n",
        "       save_results(image,mask,y_pred,save_image_path)\n",
        "\n",
        "       mask = mask/255.0\n",
        "       mask = (mask > 0.5).astype(np.int32).flatten()\n",
        "       y_pred = y_pred.flatten()\n",
        "\n",
        "       f1_value = f1_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "       jac_value = jaccard_score(mask, y_pred, labels=[0, 1], average=\"binary\")\n",
        "       recall_value = recall_score(mask, y_pred, labels=[0, 1], average=\"binary\", zero_division=0)\n",
        "       precision_value = precision_score(mask, y_pred, labels=[0, 1], average=\"binary\", zero_division=0)\n",
        "       SCORE.append([name, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "    \"\"\" Metrics values \"\"\"\n",
        "    score = [s[1:]for s in SCORE]\n",
        "    score = np.mean(score, axis=0)\n",
        "    print(f\"F1: {score[0]:0.5f}\")\n",
        "    print(f\"Jaccard: {score[1]:0.5f}\")\n",
        "    print(f\"Recall: {score[2]:0.5f}\")\n",
        "    print(f\"Precision: {score[3]:0.5f}\")\n",
        "\n",
        "    df = pd.DataFrame(SCORE, columns=[\"Image\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "    df.to_csv(\"/content/drive/MyDrive/files/score.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4jQPHlgUP69y",
        "outputId": "9dc35e8d-de83-497a-c73b-c0382e6c2de2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 612/612 [01:52<00:00,  5.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.74869\n",
            "Jaccard: 0.66662\n",
            "Recall: 0.75098\n",
            "Precision: 0.79749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNGLAkbGrwPQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}